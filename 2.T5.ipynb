{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ea84b-401d-4ced-8655-5b1bec175c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re \n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    Adafactor,\n",
    "    T5ForConditionalGeneration,\n",
    "    MT5ForConditionalGeneration,\n",
    "    #ByT5Tokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    T5TokenizerFast as T5Tokenizer,\n",
    "    MT5TokenizerFast as MT5Tokenizer,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from fastT5 import export_and_get_onnx_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e15ac0-9aca-4a59-9320-f5861730babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_name = \"claim_LOF_base_0.11_data_explanation_prep_4.pickle\"                            #\"LOF_base_0.45_0.53_removed_inlier_outlier_23.782_full.pickle\"     # \"LOF_base_0.46_0.54_removed_inlier_outlier_0.51_full.pickle\"\n",
    "data_inpit_dir = \"./Data/Selection/\"                              #\"./Data/Selection/\" \"./Data/Preprocessed/\"   \n",
    "output_dir = \"./Data/Models/\"\n",
    "source_column = \"source_text\"    #source_text_shorter           \" statement_explanation_prep\"  #\"explanation_prep\"  \"statement_explanation_prep\"                     #\"source_text_shorter\"  # \"source_text_shorter\" source_text\n",
    "target_column =  \"target_text\"              #\"shortExplanation_prep\"                         #\"target_text\"\n",
    "\n",
    "no_workers = 1\n",
    "\n",
    "imput_data_path = data_inpit_dir + input_data_name\n",
    "#input_data_name = re.sub(r'.pickle', '', input_data_name)\n",
    "new_model_name = \"d-t5-{}_{}\".format(source_column, input_data_name) # datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c07b40-4e81-4f98-ac0f-5428f42858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pl.seed_everything(42)\n",
    "\n",
    "\n",
    "class PyTorchDataModule(Dataset):\n",
    "    \"\"\"  PyTorch Dataset class  \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initiates a PyTorch Dataset Module for input data\n",
    "        Args:\n",
    "            data (pd.DataFrame): input pandas dataframe. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
    "            tokenizer (PreTrainedTokenizer): a PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n",
    "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
    "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" returns length of data \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\" returns dictionary of input tensors to feed into T5/MT5 model\"\"\"\n",
    "\n",
    "        data_row = self.data.iloc[index]\n",
    "        source_text = data_row[\"source_text\"]\n",
    "\n",
    "        source_text_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        target_text_encoding = self.tokenizer(\n",
    "            data_row[\"target_text\"],\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = target_text_encoding[\"input_ids\"]\n",
    "        labels[\n",
    "            labels == 0\n",
    "        ] = -100  # to make sure we have correct labels for T5 text generation\n",
    "\n",
    "        return dict(\n",
    "            source_text=source_text,\n",
    "            target_text=data_row[\"target_text\"],\n",
    "            source_text_input_ids=source_text_encoding[\"input_ids\"].flatten(),\n",
    "            source_text_attention_mask=source_text_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "            labels_attention_mask=target_text_encoding[\"attention_mask\"].flatten(),\n",
    "        )\n",
    "\n",
    "\n",
    "class LightningDataModule(pl.LightningDataModule):\n",
    "    \"\"\" PyTorch Lightning data class \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        batch_size: int = 4,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initiates a PyTorch Lightning Data Module\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): training dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n",
    "            test_df (pd.DataFrame): validation dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n",
    "            tokenizer (PreTrainedTokenizer): PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n",
    "            batch_size (int, optional): batch size. Defaults to 4.\n",
    "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
    "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = PyTorchDataModule(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "        self.test_dataset = PyTorchDataModule(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\" training dataloader \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers = no_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\" test dataloader \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers = no_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\" validation dataloader \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers = no_workers\n",
    "        )\n",
    "\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    \"\"\" PyTorch Lightning Model class\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, model, outputdir: str = \"outputs\"):\n",
    "        \"\"\"\n",
    "        initiates a PyTorch Lightning Model\n",
    "        Args:\n",
    "            tokenizer : T5/MT5/ByT5 tokenizer\n",
    "            model : T5/MT5/ByT5 model\n",
    "            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.outputdir = outputdir\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
    "        \"\"\" forward step \"\"\"\n",
    "        output = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_size):\n",
    "        \"\"\" training step \"\"\"\n",
    "        input_ids = batch[\"source_text_input_ids\"]\n",
    "        attention_mask = batch[\"source_text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_size):\n",
    "        \"\"\" validation step \"\"\"\n",
    "        input_ids = batch[\"source_text_input_ids\"]\n",
    "        attention_mask = batch[\"source_text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_size):\n",
    "        \"\"\" test step \"\"\"\n",
    "        input_ids = batch[\"source_text_input_ids\"]\n",
    "        attention_mask = batch[\"source_text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" configure optimizers \"\"\"\n",
    "        return Adafactor(self.parameters(), lr=0.001, scale_parameter=False, relative_step=False)\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        \"\"\" save tokenizer and model on epoch end \"\"\"\n",
    "        avg_traning_loss = np.round(\n",
    "            torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])).item(),\n",
    "            4,\n",
    "        )\n",
    "        self.avg_traning_loss = avg_traning_loss\n",
    "        path = f\"{self.outputdir}/simplet5-epoch-{self.current_epoch}-train-loss-{str(avg_traning_loss)}\"\n",
    "        print(\"Average training loss for epoch {} equal to {}\".format(self.current_epoch,self.avg_traning_loss))\n",
    "\n",
    "\n",
    "class SimpleT5:\n",
    "    \"\"\" Custom SimpleT5 class \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" initiates SimpleT5 class \"\"\"\n",
    "        pass\n",
    "\n",
    "    def from_pretrained(self, model_type=\"t5\", model_name=\"t5-base\") -> None:\n",
    "        \"\"\"\n",
    "        loads T5/MT5 Model model for training/finetuning\n",
    "        Args:\n",
    "            model_type (str, optional): \"t5\" or \"mt5\" . Defaults to \"t5\".\n",
    "            model_name (str, optional): exact model architecture name, \"t5-base\" or \"t5-large\". Defaults to \"t5-base\".\n",
    "        \"\"\"\n",
    "        if model_type == \"t5\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_name}\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                f\"{model_name}\", return_dict=True\n",
    "            )\n",
    "        elif model_type == \"mt5\":\n",
    "            self.tokenizer = MT5Tokenizer.from_pretrained(f\"{model_name}\")\n",
    "            self.model = MT5ForConditionalGeneration.from_pretrained(\n",
    "                f\"{model_name}\", return_dict=True\n",
    "            )\n",
    "        elif model_type == \"byt5\":\n",
    "            self.tokenizer = ByT5Tokenizer.from_pretrained(f\"{model_name}\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                f\"{model_name}\", return_dict=True\n",
    "            )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        eval_df: pd.DataFrame,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "        batch_size: int = 8,\n",
    "        max_epochs: int = 5,\n",
    "        use_gpu: bool = True,\n",
    "        outputdir: str = \"outputs\",\n",
    "        early_stopping_patience_epochs: int = 0,  # 0 to disable early stopping feature\n",
    "        precision=32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        trains T5/MT5 model on custom dataset\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): training datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
    "            eval_df ([type], optional): validation datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
    "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
    "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
    "            batch_size (int, optional): batch size. Defaults to 8.\n",
    "            max_epochs (int, optional): max number of epochs. Defaults to 5.\n",
    "            use_gpu (bool, optional): if True, model uses gpu for training. Defaults to True.\n",
    "            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n",
    "            early_stopping_patience_epochs (int, optional): monitors val_loss on epoch end and stops training, if val_loss does not improve after the specied number of epochs. set 0 to disable early stopping. Defaults to 0 (disabled)\n",
    "            precision (int, optional): sets precision training - Double precision (64), full precision (32) or half precision (16). Defaults to 32.\n",
    "        \"\"\"\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "        self.data_module = LightningDataModule(\n",
    "            train_df,\n",
    "            eval_df,\n",
    "            self.tokenizer,\n",
    "            batch_size=batch_size,\n",
    "            source_max_token_len=source_max_token_len,\n",
    "            target_max_token_len=target_max_token_len,\n",
    "        )\n",
    "\n",
    "        self.T5Model = LightningModel(\n",
    "            tokenizer = self.tokenizer, model=self.model, outputdir=outputdir\n",
    "        )\n",
    "\n",
    "        early_stop_callback = (\n",
    "            [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    min_delta=0.00,\n",
    "                    patience=early_stopping_patience_epochs,\n",
    "                    verbose=True,\n",
    "                    mode=\"min\",\n",
    "                )\n",
    "            ]\n",
    "            if early_stopping_patience_epochs > 0\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        gpus = 1 if use_gpu else 0\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            # logger=logger,\n",
    "            callbacks=early_stop_callback,\n",
    "            max_epochs=max_epochs,\n",
    "            gpus=gpus,\n",
    "            progress_bar_refresh_rate = 5,\n",
    "            precision=precision\n",
    "        )\n",
    "\n",
    "        trainer.fit(self.T5Model, self.data_module)\n",
    "        #path = f\"{self.T5Model.outputdir}/simplet5-epoch-{self.T5Model.current_epoch}-train-loss-{str(self.T5Model.avg_traning_loss)}\"\n",
    "        path = f\"{self.T5Model.outputdir}\" + new_model_name\n",
    "        #print(str(avg_traning_loss))\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        self.model.save_pretrained(path)\n",
    "\n",
    "    def load_model(\n",
    "        self, model_type: str = \"t5\", model_dir: str = \"outputs\", use_gpu: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        loads a checkpoint for inferencing/prediction\n",
    "        Args:\n",
    "            model_type (str, optional): \"t5\" or \"mt5\". Defaults to \"t5\".\n",
    "            model_dir (str, optional): path to model directory. Defaults to \"outputs\".\n",
    "            use_gpu (bool, optional): if True, model uses gpu for inferencing/prediction. Defaults to True.\n",
    "        \"\"\"\n",
    "        if model_type == \"t5\":\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "        elif model_type == \"mt5\":\n",
    "            self.model = MT5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = MT5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "        elif model_type == \"byt5\":\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = ByT5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda:0\")\n",
    "            else:\n",
    "                raise \"exception ---> no gpu found. set use_gpu=False, to use CPU\"\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        source_text, # str,list[str]\n",
    "        max_length_target: int = 512,\n",
    "        max_length_source: int = 1000,\n",
    "        num_return_sequences: int = 1,\n",
    "        num_beams: int = 2,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.95,\n",
    "        do_sample: bool = True,\n",
    "        repetition_penalty: float = 2.5,\n",
    "        length_penalty: float = 1.0,\n",
    "        early_stopping: bool = True,\n",
    "        skip_special_tokens: bool = True,\n",
    "        clean_up_tokenization_spaces: bool = True,\n",
    "        batch_size: int = 0\n",
    "        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        generates prediction for T5/MT5 model\n",
    "        Args:\n",
    "            source_text (str): any text for generating predictions\n",
    "            max_length_target (int, optional): max token length of prediction. Defaults to 512.\n",
    "            max_length_source (int, optional): max token length of prediction. Defaults to 512.\n",
    "            num_return_sequences (int, optional): number of predictions to be returned. Defaults to 1.\n",
    "            num_beams (int, optional): number of beams. Defaults to 2.\n",
    "            top_k (int, optional): Defaults to 50.\n",
    "            top_p (float, optional): Defaults to 0.95.\n",
    "            do_sample (bool, optional): Defaults to True.\n",
    "            repetition_penalty (float, optional): Defaults to 2.5.\n",
    "            length_penalty (float, optional): Defaults to 1.0.\n",
    "            early_stopping (bool, optional): Defaults to True.\n",
    "            skip_special_tokens (bool, optional): Defaults to True.\n",
    "            clean_up_tokenization_spaces (bool, optional): Defaults to True.\n",
    "        Returns:\n",
    "            list[str]: returns predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if batch_size == 0:\n",
    "            \n",
    "            input_ids = self.tokenizer.encode(\n",
    "                source_text, return_tensors=\"pt\", add_special_tokens=True\n",
    "            )\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length_target,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                early_stopping=early_stopping,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "            )\n",
    "            preds = [\n",
    "                self.tokenizer.decode(\n",
    "                    g,\n",
    "                    skip_special_tokens=skip_special_tokens,\n",
    "                    clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "                )\n",
    "                for g in generated_ids\n",
    "            ]\n",
    "            return preds\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            output = []\n",
    "            input_lst = [] \n",
    "            \n",
    "            for i in tqdm(range(0, len(source_text), batch_size)):\n",
    "                batch = []\n",
    "                batch = source_text[i:i + batch_size]\n",
    "                input_ids = self.tokenizer.batch_encode_plus(\n",
    "                            batch, \n",
    "                            return_tensors=\"pt\", \n",
    "                            max_length =  max_length_source, # max_length_source\n",
    "                            add_special_tokens = True, \n",
    "                            padding = \"max_length\",            #    \"max_length\",true\n",
    "                            truncation = True\n",
    "                        ).to(model.device)\n",
    "\n",
    "                hypotheses_batch = self.model.generate(\n",
    "                        **input_ids,\n",
    "                        num_beams = num_beams,\n",
    "                        max_length = max_length_target,\n",
    "                        repetition_penalty = repetition_penalty,\n",
    "                        length_penalty = length_penalty,\n",
    "                        early_stopping = early_stopping,\n",
    "                        top_p = top_p,\n",
    "                        top_k = top_k,\n",
    "                        num_return_sequences = num_return_sequences,\n",
    "                        ) #.to(model.device)\n",
    "                \n",
    "                decoded = self.tokenizer.batch_decode(\n",
    "                        hypotheses_batch, \n",
    "                        skip_special_tokens=skip_special_tokens,\n",
    "                        clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "                    )\n",
    "                \n",
    "                output.extend(decoded)\n",
    "                    \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5c545-75a1-4794-b550-a6cc25bce63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
